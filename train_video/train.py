import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
import joblib

# ============================
# 1. Load d·ªØ li·ªáu
# ============================
data = np.load("features_aug.npz")
X = data["X"]
y = data["y"]

n_samples, n_features = X.shape
n_classes = len(np.unique(y))
print(f"D·ªØ li·ªáu: {n_samples} m·∫´u, {n_features} ƒë·∫∑c tr∆∞ng, {n_classes} l·ªõp")

# ============================
# 2. Chu·∫©n h√≥a d·ªØ li·ªáu
# ============================
X_mean = X.mean(axis=0)
X_std = X.std(axis=0) + 1e-8
X = (X - X_mean) / X_std

# ============================
# 3. Chia train / test
# ============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True
)

# One-hot encoding cho nh√£n
def one_hot(y, num_classes):
    y_onehot = np.zeros((len(y), num_classes))
    y_onehot[np.arange(len(y)), y] = 1
    return y_onehot

y_train_onehot = one_hot(y_train, n_classes)
y_test_onehot = one_hot(y_test, n_classes)

# ============================
# 4. H√†m softmax
# ============================
def softmax(z):
    z -= np.max(z, axis=1, keepdims=True)  # tr√°nh tr√†n s·ªë
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# ============================
# 5. Kh·ªüi t·∫°o tham s·ªë
# ============================
np.random.seed(42)
W = np.random.randn(n_features, n_classes) * 0.01
b = np.zeros((1, n_classes))

lr = 0.1           # learning rate
num_epochs = 500   # s·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán
lambda_reg = 0.001 # h·ªá s·ªë regularization L2

train_loss_list, test_loss_list = [], []
train_acc_list, test_acc_list = [], []

# ============================
# 6. V√≤ng l·∫∑p hu·∫•n luy·ªán
# ============================
for epoch in range(num_epochs):
    # --- Forward ---
    logits_train = X_train @ W + b
    y_pred_train = softmax(logits_train)

    logits_test = X_test @ W + b
    y_pred_test = softmax(logits_test)

    # --- Loss c√≥ Regularization ---
    loss_train = -np.mean(y_train_onehot * np.log(y_pred_train + 1e-8)) + lambda_reg * np.sum(W**2)
    loss_test = -np.mean(y_test_onehot * np.log(y_pred_test + 1e-8)) + lambda_reg * np.sum(W**2)

    # --- Accuracy ---
    train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train)
    test_acc = np.mean(np.argmax(y_pred_test, axis=1) == y_test)

    # --- Gradient ---
    dW = (X_train.T @ (y_pred_train - y_train_onehot)) / len(X_train)
    dW += 2 * lambda_reg * W  # th√™m ph·∫ßn regularization
    db = np.mean(y_pred_train - y_train_onehot, axis=0, keepdims=True)

    # --- Update tr·ªçng s·ªë ---
    W -= lr * dW
    b -= lr * db

    # --- L∆∞u l·∫°i k·∫øt qu·∫£ ---
    train_loss_list.append(loss_train)
    test_loss_list.append(loss_test)
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)

    # --- In th√¥ng tin ---
    if (epoch + 1) % 10 == 0 or epoch == 0:
        print(f"Epoch {epoch+1:3d}/{num_epochs} | "
              f"TrainLoss: {loss_train:.4f} | TestLoss: {loss_test:.4f} | "
              f"TrainAcc: {train_acc:.4f} | TestAcc: {test_acc:.4f}")

# ============================
# 7. K·∫øt qu·∫£ cu·ªëi c√πng
# ============================
print("\n‚úÖ Training ho√†n t·∫•t!")
print(f"Best Train Acc: {max(train_acc_list):.4f}")
print(f"Best Test Acc:  {max(test_acc_list):.4f}")

# ============================
# 8. V·∫Ω bi·ªÉu ƒë·ªì
# ============================
plt.figure(figsize=(12, 5))

# ---- Bi·ªÉu ƒë·ªì Loss ----
plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='Train Loss')
plt.plot(test_loss_list, label='Test Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Bi·ªÉu ƒë·ªì Loss")
plt.legend()
plt.grid(True)

# ---- Bi·ªÉu ƒë·ªì Accuracy ----
plt.subplot(1, 2, 2)
plt.plot(train_acc_list, label='Train Accuracy')
plt.plot(test_acc_list, label='Test Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Bi·ªÉu ƒë·ªì Accuracy")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# ============================
# 9. ƒê√°nh gi√° m√¥ h√¨nh
# ============================
y_pred_final = np.argmax(X_test @ W + b, axis=1)
overall_acc = np.mean(y_pred_final == y_test)
print(f"\nüéØ Overall Accuracy: {overall_acc:.4f}")

# Confusion Matrix
CLASSES = ["left", "right", "yawn", "blink", "normal","nod"]  # thay t√™n theo dataset c·ªßa b·∫°n
cm = confusion_matrix(y_test, y_pred_final)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASSES)
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix")
plt.show()

# B√°o c√°o chi ti·∫øt
print("\nüîé Classification Report:")
print(classification_report(y_test, y_pred_final, target_names=CLASSES))

# ============================
# 10. L∆∞u model & scaler
# ============================
model_data = {
    "W": W,
    "b": b,
    "classes": CLASSES
}
joblib.dump(model_data, "softmax_model_best.pkl")
print("‚úÖ ƒê√£ l∆∞u model v√†o 'softmax_model_best.pkl'")

scaler = {
    "X_mean": X_mean,
    "X_std": X_std
}
joblib.dump(scaler, "scale.pkl")
print("‚úÖ ƒê√£ l∆∞u scaler v√†o 'scale.pkl'")
