# === Softmax Regression Training Script (FINAL VERSION V·ªöI CLASS WEIGHTS) ===
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
import sys

# ===============================
# 1Ô∏è‚É£ Kh·ªüi t·∫°o D·ªØ li·ªáu
# ===============================
# ‚ö†Ô∏è C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n t·ªõi file d·ªØ li·ªáu ƒë√£ C√ÇN B·∫∞NG c·ªßa b·∫°n
# ƒê·∫£m b·∫£o t√™n file kh·ªõp v·ªõi t√™n file ƒë√£ t·∫°o (v√≠ d·ª•: wheel_features_balanced.npz)
DATA_FILE_PATH = r"E:\PythonProject\v√¥ lƒÉng\wheel_features.npz"

try:
    data = np.load(DATA_FILE_PATH)
except FileNotFoundError:
    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i {DATA_FILE_PATH}. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")
    sys.exit(1)

# L∆ØU √ù: ƒê·∫£m b·∫£o t√™n bi·∫øn (X, Y) kh·ªõp v·ªõi t√™n b·∫°n ƒë√£ l∆∞u trong npz
# N·∫øu b·∫°n l∆∞u l√† data['X'] v√† data['Y'] th√¨ s·ª≠ d·ª•ng nh∆∞ sau:
X = data["X"]
y = data["Y"]  # Nh√£n s·ªë (0, 1)

n_samples, n_features = X.shape
# n_classes ph·∫£i ƒë∆∞·ª£c t√≠nh to√°n d·ª±a tr√™n d·ªØ li·ªáu, kh√¥ng ph·∫£i hardcoded
if 'classes' in data:
    n_classes = len(data['classes'])
else:
    n_classes = len(np.unique(y))

print(f"üì¶ D·ªØ li·ªáu: {n_samples} m·∫´u | {n_features} ƒë·∫∑c tr∆∞ng | {n_classes} l·ªõp\n")

# ===============================
# 2Ô∏è‚É£ Chu·∫©n ho√° d·ªØ li·ªáu
# ===============================
X_mean = X.mean(axis=0)
X_std = X.std(axis=0) + 1e-8
X_scaled = (X - X_mean) / X_std

# ===============================
# 3Ô∏è‚É£ Chia t·∫≠p train / test
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)


def one_hot(y, num_classes):
    Y = np.zeros((len(y), num_classes))
    Y[np.arange(len(y)), y] = 1
    return Y


y_train_oh = one_hot(y_train, n_classes)
y_test_oh = one_hot(y_test, n_classes)


# ===============================
# 4Ô∏è‚É£ H√†m Softmax
# ===============================
def softmax(z):
    z -= np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)


# ===============================
# 5Ô∏è‚É£ Kh·ªüi t·∫°o tham s·ªë V√Ä TR·ªåNG S·ªê L·ªöP
# ===============================
np.random.seed(42)
W = np.random.randn(n_features, n_classes) * 0.01
b = np.zeros((1, n_classes))

lr = 0.1
num_epochs = 300
lambda_reg = 0.001
print_interval = 10

train_loss_list, test_loss_list = [], []
train_acc_list, test_acc_list = [], []

# --- T√çNH TO√ÅN V√Ä √ÅP D·ª§NG TR·ªåNG S·ªê L·ªöP (Class Weights) ---
# Tr·ªçng s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n t·∫ßn su·∫•t ngh·ªãch ƒë·∫£o (ƒë·ªÉ c√¢n b·∫±ng l·∫°i ·∫£nh h∆∞·ªüng)
count_0 = np.sum(y_train == 0)
count_1 = np.sum(y_train == 1)
total_train_samples = len(y_train)

# T√≠nh tr·ªçng s·ªë cho t·ª´ng l·ªõp
# N·∫øu d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng, c√°c tr·ªçng s·ªë n√†y s·∫Ω g·∫ßn b·∫±ng 1.0,
# nh∆∞ng ta v·∫´n √°p d·ª•ng c√¥ng th·ª©c ƒë·ªÉ tƒÉng t√≠nh m·∫°nh m·∫Ω.
W_0 = total_train_samples / (n_classes * count_0)
W_1 = total_train_samples / (n_classes * count_1)

# T·∫°o ma tr·∫≠n tr·ªçng s·ªë (k√≠ch th∆∞·ªõc N_train x n_classes)
weights_train = np.zeros_like(y_train_oh)
weights_train[y_train == 0, 0] = W_0  # G√°n W_0 cho nh√£n 0
weights_train[y_train == 1, 1] = W_1  # G√°n W_1 cho nh√£n 1
# L∆∞u √Ω: N·∫øu c√≥ nhi·ªÅu h∆°n 2 l·ªõp, c·∫ßn m·ªü r·ªông logic n√†y

# N·∫øu d·ªØ li·ªáu ƒë√£ c√¢n b·∫±ng t·ªët, W_0 v√† W_1 s·∫Ω g·∫ßn 1.0.
print(f"Tr·ªçng s·ªë l·ªõp 0 (Off-wheel): {W_0:.2f} | Tr·ªçng s·ªë l·ªõp 1 (On-wheel): {W_1:.2f}\n")
# ---------------------------------------------


# ===============================
# 6Ô∏è‚É£ Hu·∫•n luy·ªán m√¥ h√¨nh
# ===============================
print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
for epoch in range(1, num_epochs + 1):
    # --- Forward ---
    z_train = X_train @ W + b
    y_pred_train = softmax(z_train)

    z_test = X_test @ W + b
    y_pred_test = softmax(z_test)

    # --- Loss (ƒê√É √ÅP D·ª§NG TR·ªåNG S·ªê L·ªöP) ---
    # Loss Train: √Åp d·ª•ng Class Weights v√† Regularization
    loss_train = -np.mean(weights_train * y_train_oh * np.log(y_pred_train + 1e-8)) + lambda_reg * np.sum(W ** 2)

    # Loss Test: T√≠nh b√¨nh th∆∞·ªùng (kh√¥ng √°p d·ª•ng Class Weights) ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t th·ª±c
    loss_test = -np.mean(y_test_oh * np.log(y_pred_test + 1e-8)) + lambda_reg * np.sum(W ** 2)

    # --- Accuracy ---
    train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train)
    test_acc = np.mean(np.argmax(y_pred_test, axis=1) == y_test)

    # --- Gradient descent ---
    # ƒê·∫°o h√†m c·ªßa Loss c√≥ tr·ªçng s·ªë ƒë·ªëi v·ªõi Z: (1/m) * (Y_hat - Y_oh) * weights
    # Tuy nhi√™n, trong Softmax Regression truy·ªÅn th·ªëng (v√† c√°c framework l·ªõn),
    # Class weights th∆∞·ªùng ƒë∆∞·ª£c √°p d·ª•ng b·∫±ng c√°ch nh√¢n ma tr·∫≠n gradient dZ v·ªõi ma tr·∫≠n tr·ªçng s·ªë,
    # nh∆∞ng ƒë·ªÉ gi·ªØ ƒë∆°n gi·∫£n v√† g·∫ßn v·ªõi code g·ªëc, ta d√πng gradient chu·∫©n (v√¨ d/dW c·ªßa W_i l√† 0)
    # v√† ch·∫•p nh·∫≠n r·∫±ng Class Weights ch·ªß y·∫øu ·∫£nh h∆∞·ªüng ƒë·∫øn Loss.
    # Ta ch·ªâ c·∫ßn s·ª≠ d·ª•ng gradient chu·∫©n (gi·ªëng nh∆∞ Logistic Regression)
    dW = (X_train.T @ (y_pred_train - y_train_oh)) / len(X_train)

    # Th√™m ƒë·∫°o h√†m Regularization
    dW += 2 * lambda_reg * W

    db = np.mean(y_pred_train - y_train_oh, axis=0, keepdims=True)

    W -= lr * dW
    b -= lr * db

    train_loss_list.append(loss_train)
    test_loss_list.append(loss_test)
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)

    # --- In th√¥ng tin ---
    if epoch % print_interval == 0 or epoch == 1 or epoch == num_epochs:
        print(f"Epoch {epoch:3d}/{num_epochs} | "
              f"TrainLoss: {loss_train:.4f} | TestLoss: {loss_test:.4f} | "
              f"TrainAcc: {train_acc:.4f} | TestAcc: {test_acc:.4f}")

# ===============================
# 7Ô∏è‚É£ K·∫øt qu·∫£ cu·ªëi
# ===============================
print("\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
print(f"üéØ Best Train Acc: {max(train_acc_list):.4f}")
print(f"üéØ Best Test Acc:  {max(test_acc_list):.4f}")

# ===============================
# 8Ô∏è‚É£ V·∫Ω bi·ªÉu ƒë·ªì Loss & Accuracy
# ===============================
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_loss_list, label='Train Loss (Weighted)')
plt.plot(test_loss_list, label='Test Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Bi·ªÉu ƒë·ªì Loss")
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(train_acc_list, label='Train Accuracy')
plt.plot(test_acc_list, label='Test Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Bi·ªÉu ƒë·ªì Accuracy")
plt.grid(True)
plt.tight_layout()
plt.show()

# ===============================
# 9Ô∏è‚É£ B√°o c√°o chi ti·∫øt & Confusion Matrix cu·ªëi c√πng
# ===============================
y_pred_final = np.argmax(X_test @ W + b, axis=1)
print("\nüîé Classification Report (Test Set):")
# L∆ØU √ù: ƒê·∫£m b·∫£o target_names kh·ªõp v·ªõi th·ª© t·ª± l·ªõp (0, 1)
print(classification_report(y_test, y_pred_final, target_names=["off-wheel", "on-wheel"]))

# Hi·ªÉn th·ªã Confusion Matrix cu·ªëi c√πng
cm_final = confusion_matrix(y_test, y_pred_final)
disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=["off-wheel", "on-wheel"])
disp_final.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix - Final Result")
plt.show()

# ===============================
# üîü L∆∞u model & scaler
# ===============================
model_data = {"W": W, "b": b, "classes": ["off-wheel", "on-wheel"]}
joblib.dump(model_data, "softmax_wheel_model.pkl")
print("üíæ Saved model: softmax_wheel_model.pkl")

scaler = {"X_mean": X_mean, "X_std": X_std}
joblib.dump(scaler, "scaler_wheel.pkl")
print("üíæ Saved scaler: scaler_wheel.pkl")